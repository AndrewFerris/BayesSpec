{
    "collab_server" : "",
    "contents" : "#' @name adaptspec\n#'\n#' @title Adaptive Spectral Estimation for Non-stationary Time Series\n#'\n#' @description Methodology for analyzing possibly non-stationary time series by adaptively dividing the time series into an unknown but finite number of segments and estimating the corresponding local spectra by smoothing splines.\n#'\n#' @param nloop The total number of MCMC iterations\n#' @param nwarmup The number of burn-in iterations\n#' @param nexp_max The maximum number of segments allowed\n#' @param x The data, a univariate time series, not a time series object\n#'\n#' @param tmin The minimum number of observations per segment. An optional argument defaulted to tmin = 40.\n#' @param sigmasqalpha An optional argument defaulted to sigmasqalpha = 100.\n#' @param tau_prior_a An optional argurment defaulted to tau_prior_a = -1.\n#' @param tau_prior_b An optional argurment defaulted to tau_prior_b = 0.\n#' @param tau_up_limit An optional argurment defaulted to tau_up_limit = 10000.\n#' @param prob_mm1 An optional argurment defaulted to prob_mm1 = 0.8.\n#' @param step_size_max An optional argurment defaulted to step_size_max = 10.\n#' @param var_inflate An optional argurment defaulted to var_inflate = 1.\n#' @param nbasis An optional argurment defaulted to nbasis = 7.\n#' @param nfreq_hat An optional argurment defaulted to nfreq_hat = 50.\n#' @param plotting An optional argument for displaying output plots defaulted to FALSE. When set to TRUE, this displays the spectral and parition points.\n#'\n#' @return xi The partition points\n#' @return log_spec_hat Estimates of the log spectra for all segments\n#' @return nexp_curr The number of segments in each iteration.\n#'\n#' @import mvtnorm pscl trust\n#'\n#' @usage\n#' adaptspec(nloop, nwarmup, nexp_max, x,\n#'    tmin, sigmasqalpha, tau_prior_a, tau_prior_b,\n#'    tau_up_limit, prob_mm1, step_size_max,\n#'    var_inflate, nbasis, nfreq_hat, plotting)\n#'\n#' @examples\n#' #Running adaptspec with the simulated_piecewise data.\n#' data(simulated_piecewise)\n#' model1 <- adaptspec(nloop = 80, nwarmup = 20,\n#'    nexp_max = 5, x = simulated_piecewise[1:100])\n#' str(model1)\n#' summary(model1$nexp_curr)\n#' plot(model1$nexp_curr)\n#'\n#' @author Rosen, O., Wood, S. and Stoffer, D.\n#'\n#' @references Rosen, O., Wood, S. and Stoffer, D. (2012). AdaptSPEC: Adaptive Spectral Estimation for Nonstationary Time Series. J. of the American Statistical Association, 107, 1575-1589\n#'\n#' @export\nNULL\n\nadaptspec <- function(nloop, nwarmup, nexp_max, x, tmin, sigmasqalpha, tau_prior_a, tau_prior_b, tau_up_limit, prob_mm1, step_size_max, var_inflate, nbasis, nfreq_hat, plotting){\n\n#For optional variables,\nif(missing(sigmasqalpha)){\n  sigmasqalpha <- 100\n} else{\n  sigmasqalpha\n}\n\nif(missing(tau_prior_a)){\n  tau_prior_a <- -1\n} else{\n  tau_prior_a\n}\n\nif(missing(tau_prior_b)){\n  tau_prior_b <- 0\n} else{\n  tau_prior_b\n}\n\nif(missing(tau_up_limit)){\n  tau_up_limit <- 10000\n} else{\n  tau_up_limit\n}\n\nif(missing(prob_mm1)){\n  prob_mm1 <- 0.8\n} else{\n  prob_mm1\n}\n\nif(missing(step_size_max)){\n  step_size_max <- 10\n} else{\n  step_size_max\n}\n\nif(missing(var_inflate)){\n  var_inflate <- 1\n} else{\n  var_inflate\n}\n\nif(missing(nbasis)){\n  nbasis <- 7\n} else{\n  nbasis\n}\n\nif(missing(nfreq_hat)){\n  nfreq_hat <- 50\n} else{\n  nfreq_hat\n}\n\nif(missing(tmin)){\n  tmin <- 40\n} else{\n  tmin\n}\n\nif(missing(plotting)){\n  plotting <- FALSE\n} else{\n  plotting\n}\n\nif(plotting != TRUE){\n  plotting <- FALSE\n} else{\n  plotting\n}\n\nlin_basis_func <- function(freq, nbeta){\n  nbasis <- nbeta-1\n  n <- length(freq)\n  omega <- matrix(0,n,nbasis)\n  for (j in 1:nbasis){\n    omega[,j] <- sqrt(2)*cos(2*j*pi*freq)/(2*pi*j)\n  }\n  return(cbind(rep(1,n),omega))\n}\n\nbeta_derivs <- function(param,n,xx,A,precs){\n   n1 <- floor(n/2)\n  xxb <- xx %*% param\n  Ae <- as.vector(A*exp(-xxb))\n  if (n %% 2 == 1){ #odd n\n    f <- -sum(xxb[2:(n1+1)]+Ae[2:(n1+1)]) -.5*(xxb[1] + Ae[1]) -\n      .5*t(param) %*% diag(precs) %*% param\n    g <- -t(xx[2:(n1+1),]) %*% (1-Ae[2:(n1+1)]) -\n      .5*(1-Ae[1])*xx[1,] - diag(precs) %*% param\n    h <- -t(xx[2:(n1+1),]) %*% diag(Ae[2:(n1+1)]) %*% xx[2:(n1+1),]\n    -.5*Ae[1] * xx[1,] %*% t(xx[1,]) - diag(precs)\n  }else { # even n\n    f <- -sum(xxb[2:n1]+Ae[2:n1]) -.5*(xxb[1] + Ae[1]) -\n      .5*(xxb[n1+1] + Ae[n1+1]) - .5*t(param) %*% diag(precs) %*% param\n    g <- -t(xx[2:n1,]) %*% (1-Ae[2:n1]) - .5*(1-Ae[1])*xx[1,] -\n      .5*(1-Ae[n1+1])*xx[n1+1,] - diag(precs) %*% param\n    h <- -t(xx[2:n1,]) %*% diag(Ae[2:n1]) %*% xx[2:n1,] -.5*Ae[1] * xx[1,] %*% t(xx[1,]) -\n      .5*Ae[n1+1] * xx[n1+1,] %*% t(xx[n1+1,]) - diag(precs)\n  }\n  list(value = f, gradient = g, hessian = h)\n}\n\npost_beta <- function(j, nseg_temp, x, xi_temp, tau_temp,\n                      token_information){\n  #   j=1 #used for debugging\n  #   nseg_temp = nseg_temp[j] # used for debugging\n  #   tau_temp = tau_temp[j] # used for debugging\n  var_inflate <- token_information$var_inflate\n  nbeta <- token_information$nbeta\n  nbasis <- token_information$nbasis\n  sigmasqalpha <- token_information$sigmasqalpha\n  nfreq <- floor(nseg_temp/2)\n  freq <- (0:nfreq)/(2*nfreq)\n  if (j>1){\n    dft <- fft(x[(xi_temp[j-1]+1):xi_temp[j]])/sqrt(nseg_temp)\n    y <- dft[1:(nfreq+1)]\n    prdgrm <- abs(y)^2\n  } else {\n    dft <- fft(x[1:xi_temp[j]])/sqrt(nseg_temp)\n    y <- dft[1:(nfreq+1)]\n    prdgrm <- abs(y)^2\n  }\n  nu_mat <- lin_basis_func(freq, nbeta)\n  nn <- nseg_temp\n  ytemp <- prdgrm\n  param<-rep(0,nbeta)\n  precs <- c(1/sigmasqalpha,rep(1/tau_temp,nbasis))\n  opt <- trust(beta_derivs, param, rinit=1, rmax=100, parscale=rep(1,nbeta),\n               iterlim = 100, fterm = sqrt(.Machine$double.eps),\n               mterm = sqrt(.Machine$double.eps),\n               minimize = FALSE, blather = FALSE, nn, nu_mat, prdgrm, precs)\n  beta_mean <- opt$argument\n  beta_var <- -solve(opt$hessian)\n  list(beta_mean = beta_mean, beta_var = beta_var, nu_mat = nu_mat, prdgrm = prdgrm)\n}\n\nwhittle_like <- function(dd,fhat,n){\n  #fhat is log spec density\n  # A is the periodogram\n  # dd=fit #debugging\n  # n=nseg_curr_temp[j] #debugging\n  A <- dd$prdgrm\n  fhat <- as.vector(fhat)\n  Ae <- as.vector(A*exp(-fhat))\n  n1 <- floor(n/2)\n  if (n %% 2 == 1){ #odd n\n    f <- -sum(fhat[2:(n1+1)]+Ae[2:(n1+1)]) -.5*(fhat[1] + Ae[1]) -\n      .5*n*log(2*pi)\n  }else { # even n\n    f <- -sum(fhat[2:n1]+Ae[2:n1]) -.5*(fhat[1] + Ae[1]) -\n      .5*(fhat[n1+1] + Ae[n1+1]) - .5*n*log(2*pi)\n  }\n  return(f)\n}\n\nwithin = function (x,nexp_temp,xi_curr_temp,beta_curr_temp,nseg_curr_temp,tau_temp,token_information){\n  # nexp_temp = nexp_curr[p+1] #just for debugging\n  xi_prop <- xi_curr_temp\n  beta_prop <- beta_curr_temp\n  nseg_new <- nseg_curr_temp\n  nobs <- token_information$nobs\n  nbeta <- token_information$nbeta\n  nbasis <- token_information$nbasis\n  sigmasqalpha <- token_information$sigmasqalpha\n  prob_mm1 <- token_information$prob_mm1\n\n  if (nexp_temp>1) {\n    seg_temp <- sample(1:(nexp_temp-1),1,replace = TRUE) #Drawing Segment to relocate cutpoint\n    u <- runif(1)\n    cut_poss_curr <- xi_curr_temp[seg_temp]\n    nposs_prior <- nseg_curr_temp[seg_temp]+nseg_curr_temp[seg_temp+1]-2*tmin+1\n    if (u < prob_mm1){\n      if (nseg_curr_temp[seg_temp] == tmin & nseg_curr_temp[seg_temp+1] == tmin) {\n        nposs <- 1 # Number of possible locations for new cutpoint\n        new_index<-sample(1:nposs, 1, replace = TRUE) #Drawing index of new cutpoint\n        cut_poss_new <- xi_curr_temp[seg_temp]-1+new_index\n      } else if (nseg_curr_temp[seg_temp] == tmin) {\n        nposs <- 2 # Number of possible locations for new cutpoint\n        new_index <- sample(1:nposs, 1, replace = TRUE) #Drawing index of new cutpoint\n        cut_poss_new <- xi_curr_temp[seg_temp]-1+new_index\n      } else if (nseg_curr_temp[seg_temp+1] == tmin) {\n        nposs <- 2 # Number of possible locations for new cutpoint\n        new_index <- sample(1:nposs,1,replace = TRUE) #Drawing index of new cutpoint\n        cut_poss_new <- xi_curr_temp[seg_temp]+1-new_index\n      } else {\n        nposs <- 3 # Number of possible locations for new cutpoint\n        new_index<-sample(1:nposs, 1, replace = TRUE) # Drawing index of new cutpoint\n        cut_poss_new<-xi_curr_temp[seg_temp]-2+new_index\n      }\n    } else{ # u not < prob_mm1\n      new_index <- sample(1:nposs_prior,1,replace = TRUE)\n      if (seg_temp > 1){\n        cut_poss_new <- sum(nseg_curr_temp[1:(seg_temp-1)])-1+tmin+new_index\n      } else {\n        cut_poss_new <- -1+tmin+new_index\n      }\n    }\n    xi_prop[seg_temp] <- cut_poss_new\n    if(seg_temp>1){\n      nseg_new[seg_temp] <- xi_prop[seg_temp]-xi_curr_temp[seg_temp-1] #Number of observations in lower part of new cutpoint\n    } else {\n      nseg_new[seg_temp] <- xi_prop[seg_temp]\n    }\n    nseg_new[seg_temp+1] <- nseg_curr_temp[seg_temp]+nseg_curr_temp[seg_temp+1]-nseg_new[seg_temp] #Number of observations in upper part of new cutpoint\n    # Evaluating the Proposal density for the cut-point at the cureent and proposed values\n    if(abs(cut_poss_new-cut_poss_curr)>1){\n      log_prop_cut_prop <- log(1-prob_mm1) - log(nposs_prior)\n      log_prop_cut_curr <- log(1-prob_mm1) - log(nposs_prior)\n    } else if (nseg_curr_temp[seg_temp] == tmin & nseg_curr_temp[seg_temp+1] == tmin){\n      log_prop_cut_prop <- 0\n      log_prop_cut_curr <- 0\n    } else {\n      if (nseg_curr_temp[seg_temp] == tmin || nseg_curr_temp[seg_temp+1] == tmin) {\n        log_prop_cut_prop <- log(1-prob_mm1)-log(nposs_prior)+log(1/2)+log(prob_mm1)\n      } else {\n        log_prop_cut_prop <- log(1-prob_mm1)-log(nposs_prior)+log(1/3)+log(prob_mm1)\n      }\n      if (nseg_new[seg_temp] == tmin || nseg_new[seg_temp+1] == tmin) {\n        log_prop_cut_curr <- log(1-prob_mm1)-log(nposs_prior)+log(1/2)+log(prob_mm1)\n      } else {\n        log_prop_cut_curr<-log(1-prob_mm1)-log(nposs_prior)+log(1/3)+log(prob_mm1)\n      }\n    }\n    #Evaluating the Loglikelihood, Priors and Proposals at the\n    #current values\n    loglike_curr <- 0\n    log_beta_curr_temp <- 0\n    log_prior_curr <- 0\n    for (j in seg_temp:(seg_temp+1)){\n      fit <- post_beta(j,nseg_curr_temp[j],x,xi_curr_temp,tau_temp[j], token_information)\n      #Compute log proposal density of beta at current  values\n      log_beta_curr_temp <- log_beta_curr_temp+dmvnorm(beta_curr_temp[,j], fit$beta_mean, fit$beta_var, log = TRUE)\n      fhat <- fit$nu_mat %*% beta_curr_temp[,j]\n      #Compute Loglike at current values\n      # cat(\"fhat:\", fhat, \"\\n\")\n      log_curr_spec_dens <- whittle_like(fit, fhat,nseg_curr_temp[j])\n      #       Ae = as.vector(fit$prdgrm*exp(-as.vector(fhat)))\n      #       log_curr_spec_dens = -sum(fhat+Ae)\n      # cat(\"log_curr_spec_dens:\", log_curr_spec_dens, \"\\n\")\n      loglike_curr<-loglike_curr+log_curr_spec_dens\n      #Compute priors at current values\n      log_prior_curr <- log_prior_curr +\n        dmvnorm(beta_curr_temp[,j], rep(0,nbeta), diag(c(sigmasqalpha,rep(tau_temp[j],nbasis))), log = TRUE)\n    }\n    #Evaluating the Loglikelihood, Priors and Proposals at the\n    #proposed values Likelihood\n    loglike_prop <- 0\n    log_beta_prop <- 0\n    log_prior_prop <- 0\n    for (j in seg_temp:(seg_temp+1)){\n      fit <- post_beta(j,nseg_new[j],x,xi_prop,tau_temp[j], token_information)\n      beta_prop[,j]<-rmvnorm(1,fit$beta_mean,fit$beta_var)\n      # Compute log proposal density of beta at proposed\n      # values\n      log_beta_prop <- log_beta_prop+dmvnorm(beta_prop[,j],fit$beta_mean, fit$beta_var, log = TRUE)\n      fhat<-fit$nu_mat %*% beta_prop[,j]\n      #Compute Loglike at proposed values\n      log_prop_spec_dens <- whittle_like(fit,fhat,nseg_new[j])\n      # cat(\"log_prop_spec_dens:\", log_prop_spec_dens, \"\\n\")\n      loglike_prop <- loglike_prop + log_prop_spec_dens\n      # Compute priors at proposed values\n      log_prior_prop <- log_prior_prop +\n        dmvnorm(beta_prop[,j], rep(0,nbeta), diag(c(sigmasqalpha,rep(tau_temp[j],nbasis))), log = TRUE)\n    }\n    #Proposal for beta\n    log_proposal_curr <- log_beta_curr_temp+log_prop_cut_curr\n    log_proposal_prop <- log_beta_prop+log_prop_cut_prop\n    log_prior_cut_prop <- 0\n    log_prior_cut_curr <- 0\n    for (k in 1:(nexp_temp-1)){\n      if (k==1){\n        log_prior_cut_prop <- -log(nobs-(nexp_temp-k+1)*tmin+1)\n        log_prior_cut_curr <- -log(nobs-(nexp_temp-k+1)*tmin+1)\n      } else {\n        log_prior_cut_prop <- log_prior_cut_prop-log(nobs-xi_prop[k-1]-(nexp_temp-k+1)*tmin+1)\n        log_prior_cut_curr <- log_prior_cut_curr-log(nobs-xi_curr_temp[k-1]-(nexp_temp-k+1)*tmin+1)\n      }\n    }\n    log_target_prop <- loglike_prop+log_prior_prop+log_prior_cut_prop\n    log_target_curr <- loglike_curr+log_prior_curr+log_prior_cut_curr\n  } else { #nexp_temp not greater than 1\n    nseg_new <- nobs\n    seg_temp <- 1\n    fit <- post_beta(1,nobs,x,xi_prop,tau_temp, token_information)\n    beta_prop <- t(rmvnorm(1,fit$beta_mean,fit$beta_var))\n    #Compute log proposal density of beta at proposed  values\n    log_beta_prop <- dmvnorm(as.vector(beta_prop),fit$beta_mean,fit$beta_var, log = TRUE)\n    #Compute log proposal density of beta at current  values\n    log_beta_curr_temp <- dmvnorm(beta_curr_temp,fit$beta_mean,fit$beta_var, log = TRUE)\n    #Compute Loglike at proposed values\n    fhat <- fit$nu_mat %*% beta_prop\n    loglike_prop <- whittle_like(fit,fhat,nobs)\n    # cat(\"loglike_prop:\", loglike_prop,\"\\n\")\n    #Compute Loglike at current values\n    fhat <- fit$nu_mat %*% beta_curr_temp\n    loglike_curr <- whittle_like(fit,fhat,nobs)\n    # cat(\"loglike_curr:\", loglike_curr,\"\\n\")\n    #Compute Priors at proposed values\n    log_prior_prop <- dmvnorm(t(beta_prop), rep(0,nbeta), diag(c(sigmasqalpha,rep(tau_temp,nbasis))), log = TRUE)\n    #Compute Priors at current values\n    log_prior_curr <- dmvnorm(beta_curr_temp, rep(0,nbeta), diag(c(sigmasqalpha,rep(tau_temp,nbasis))), log = TRUE)\n    log_proposal_curr <- log_beta_curr_temp\n    log_proposal_prop <- log_beta_prop\n    log_target_prop <- loglike_prop+log_prior_prop\n    log_target_curr <- loglike_curr+log_prior_curr\n  } #end else {nexp_temp not greater than 1\n  epsilon <- min(1,exp(log_target_prop-log_target_curr+log_proposal_curr-log_proposal_prop))\n  list(epsilon = epsilon, xi_prop = xi_prop, beta_prop = beta_prop,\n       nseg_new = nseg_new, seg_temp = seg_temp)\n} #end func\n\ndeath <- function(x,nexp_curr_temp,nexp_prop,tau_curr_temp,xi_curr_temp,nseg_curr_temp,\n                  beta_curr_temp,log_move_curr,log_move_prop,token_information){\n\n  # nexp_curr_temp <- nexp_curr[p] # just for debugging\n  nobs <- token_information$nobs\n  nbeta <- token_information$nbeta\n  nbasis <- token_information$nbasis\n  sigmasqalpha <- token_information$sigmasqalpha\n  tmin <- token_information$tmin\n  tau_up_limit <- token_information$tau_up_limit\n  beta_prop <- matrix(0,nbeta,nexp_prop)\n  tau_prop <- matrix(1,nexp_prop,1)\n  nseg_prop <- matrix(0,nexp_prop,1)\n  xi_prop <- matrix(0,nexp_prop,1)\n  # Drawing  cut_point to delete\n  cut_del <- sample(1:(nexp_curr_temp-1), 1, replace = TRUE)\n  j <- 0\n  for (k in 1:nexp_prop){\n    j <- j+1\n    if (k == cut_del){\n      #*************************************************************\n      # PROPOSED VALUES\n      #*************************************************************\n      xi_prop[k] <- xi_curr_temp[j+1]\n      tau_prop[k] <- sqrt(tau_curr_temp[j]*tau_curr_temp[j+1]) # Combining 2 taus into 1\n      nseg_prop[k] <- nseg_curr_temp[j]+nseg_curr_temp[j+1] # Combining two segments into 1\n      #==================================================================\n      # Evaluating the Likelihood, Proposal and Prior Densities at the Proposed values\n      #==================================================================\n      #Computing mean and variances for beta proposals\n      fit <- post_beta(k,nseg_prop[k],x,xi_prop,tau_prop[k],token_information)\n      beta_prop[,k] <- rmvnorm(1,fit$beta_mean,fit$beta_var)\n      # Loglikelihood  at proposed values\n      fhat <- fit$nu_mat %*% beta_prop[,k]\n      # Ae = as.vector(fit$prdgrm*exp(-as.vector(fhat)))\n      # log_prop_spec_dens = -sum(fhat+Ae)\n      log_prop_spec_dens <- whittle_like(fit, fhat, nseg_prop[k])\n      loglike_prop <- log_prop_spec_dens\n      #==================================================================\n      #Evaluating the Proposal Densities at the Proposed values of beta,\n      #the cut points\n      #==================================================================\n      #Beta\n      log_beta_prop <- dmvnorm(beta_prop[,k],fit$beta_mean, fit$beta_var, log = TRUE)\n      #Segment\n      log_seg_prop <- -log(nexp_curr_temp-1)\n      # Calcualting Jacobian\n      log_jacobian <- -log(2*(sqrt(tau_curr_temp[j])+sqrt(tau_curr_temp[j+1]))^2)\n      #Calculating Log Proposal density at Proposed values\n      log_proposal_prop <- log_beta_prop+log_seg_prop+log_move_prop\n      #==================================================================\n      #Evaluating the Prior Densities at the Proposed values for tau and beta\n      #==============================================================\n      # Beta\n      log_beta_prior_prop <- dmvnorm(beta_prop[,k], rep(0,nbeta),\n                                     diag(c(sigmasqalpha,rep(tau_prop[k],nbasis))), log = TRUE)\n      # Tau\n      log_tau_prior_prop <- -log(tau_up_limit)\n      #*************************************************************\n      # CURRENT VALUES\n      #*************************************************************\n      #=======================================\n      # Evaluating the Likelihood, Proposal and Prior Densities at the Current values\n      #=======================================\n      #Beta Proposal and Prior\n      log_beta_curr <- 0\n      log_tau_prior_curr <- 0\n      log_beta_prior_curr <- 0\n      loglike_curr <- 0\n      for (jj in j:(j+1)){\n        fit <- post_beta(jj,nseg_curr_temp[jj],x,xi_curr_temp,tau_curr_temp[jj],token_information)\n        log_beta_curr <- log_beta_curr+dmvnorm(beta_curr_temp[,jj],fit$beta_mean, fit$beta_var, log = TRUE)\n        log_beta_prior_curr <- log_beta_prior_curr+dmvnorm(beta_curr_temp[,jj], rep(0,nbeta),\n                                                           diag(c(sigmasqalpha,rep(tau_curr_temp[jj],nbasis))), log = TRUE)\n        log_tau_prior_curr <- log_tau_prior_curr-log(tau_up_limit)\n        # Loglikelihood  at Current values\n        fhat <- fit$nu_mat %*% beta_curr_temp[,jj]\n        # Ae = as.vector(fit$prdgrm*exp(-as.vector(fhat)))\n        # log_curr_spec_dens = -sum(fhat+Ae)\n        log_curr_spec_dens <- whittle_like(fit, fhat, nseg_curr_temp[jj])\n        loglike_curr <- loglike_curr+log_curr_spec_dens\n      }\n      # Calculating Log proposal density at current values\n      log_proposal_curr <- log_move_curr+log_beta_curr\n      # Calculating Priors at Current Vlaues\n      log_prior_curr <- log_beta_prior_curr+log_tau_prior_curr\n      j <- j+1\n    } else {\n      xi_prop[k] <- xi_curr_temp[j]\n      tau_prop[k] <- tau_curr_temp[j]\n      nseg_prop[k] <- nseg_curr_temp[j]\n      beta_prop[,k] <- beta_curr_temp[,j]\n    } # end  if (k == cut_del)\n  } # end for (k in 1:nexp_prop)\n  #Evaluating Target density at proposed values\n  log_prior_cut_prop <- 0\n  for (k in 1:(nexp_prop-1)){\n    if (k==1){\n      log_prior_cut_prop <- -log(nobs-(nexp_prop-k+1)*tmin+1)\n    } else {\n      log_prior_cut_prop <- log_prior_cut_prop-log(nobs-xi_prop[k-1]-(nexp_prop-k+1)*tmin+1)\n    }\n  }\n  log_target_prop <- loglike_prop+log_tau_prior_prop+log_beta_prior_prop+log_prior_cut_prop\n  #Evaluating Target density at current values\n  log_prior_cut_curr <- 0\n  for (k in 1:(nexp_curr_temp-1)){\n    if (k==1) {\n      log_prior_cut_curr <- -log(nobs-(nexp_curr_temp-k+1)*tmin+1)\n    } else {\n      log_prior_cut_curr <- log_prior_cut_curr-log(nobs-xi_curr_temp[k-1]-(nexp_curr_temp-k+1)*tmin+1)\n    }\n  }\n  log_target_curr <- loglike_curr+log_prior_curr+log_prior_cut_curr\n  met_rat <- min(1,exp(log_target_prop-log_target_curr+log_proposal_curr-log_proposal_prop+log_jacobian))\n  list(met_rat = met_rat,nseg_prop = nseg_prop,xi_prop =xi_prop,tau_prop = tau_prop,\n       beta_prop = beta_prop)\n} # end function\n\nbirth <- function(x,nexp_curr,nexp_prop,tau_curr_temp,xi_curr_temp,nseg_curr_temp,beta_curr_temp,\n                  log_move_curr,log_move_prop,token_information){\n  # nexp_curr <- nexp_curr[p] # just for debugging\n  nobs <- token_information$nobs\n  nbeta <- token_information$nbeta\n  nbasis <- token_information$nbasis\n  sigmasqalpha <- token_information$sigmasqalpha\n  tmin <- token_information$tmin\n  tau_up_limit <- token_information$tau_up_limit\n  beta_prop <- matrix(0,nbeta,nexp_prop)\n  tau_prop <- matrix(1,nexp_prop,1)\n  nseg_prop <- matrix(0,nexp_prop,1)\n  xi_prop <- matrix(0,nexp_prop,1)\n  #Drawing  segment to split\n  kk <- which(nseg_curr_temp > 2*tmin) #Number of segments available for splitting\n  nposs_seg <- length(kk)\n  seg_cut <- kk[sample(1:nposs_seg, 1, replace = TRUE)] #Drawing segment to split\n  nposs_cut <- nseg_curr_temp[seg_cut]-2*tmin+1 # Drawing New cutpoint\n  for (jj in 1:nexp_curr){\n    if (jj < seg_cut){\n      xi_prop[jj] <- xi_curr_temp[jj]\n      tau_prop[jj] <- tau_curr_temp[jj]\n      nseg_prop[jj] <- nseg_curr_temp[jj]\n      beta_prop[,jj] <- beta_curr_temp[,jj]\n    } else if (jj == seg_cut){\n      index <- sample(1:nposs_cut, 1, replace = TRUE)\n      if (seg_cut==1){\n        xi_prop[seg_cut] <- index+tmin-1\n      } else{\n        xi_prop[seg_cut] <- xi_curr_temp[jj-1]-1+tmin+index\n      }\n      xi_prop[seg_cut+1] <- xi_curr_temp[jj]\n      zz <- runif(1) #Drawing new tausq\n      tau_prop[seg_cut] <- tau_curr_temp[seg_cut]*zz/(1-zz)\n      tau_prop[seg_cut+1] <- tau_curr_temp[seg_cut]*(1-zz)/zz\n      nseg_prop[seg_cut] <- index+tmin-1\n      nseg_prop[seg_cut+1] <- nseg_curr_temp[jj]-nseg_prop[seg_cut]\n      for (k in jj:(jj+1)){\n        fit <- post_beta(k,nseg_prop[k],x,xi_prop,tau_prop[k], token_information)\n        beta_prop[,k] <- rmvnorm(1,fit$beta_mean,fit$beta_var) #Drawing a new value of beta\n      }\n    } else{\n      xi_prop[jj+1] <- xi_curr_temp[jj]\n      tau_prop[jj+1] <- tau_curr_temp[jj]\n      nseg_prop[jj+1] <- nseg_curr_temp[jj]\n      beta_prop[,jj+1] <- beta_curr_temp[,jj]\n    } # end if (jj < seg_cut)\n  } # end jj in 1:nexp_curr\n  #Calculating Jacobian\n  log_jacobian <- log(2*tau_curr_temp[seg_cut]/(zz*(1-zz)))\n  #=======================================\n  # Evaluating the Likelihood, Proposal and Prior Densities at the Proposed values\n  #=======================================\n  log_beta_prop <- 0\n  log_tau_prior_prop <- 0\n  log_beta_prior_prop <- 0\n  loglike_prop <- 0\n  for (jj in seg_cut:(seg_cut+1)){\n    fit <- post_beta(jj,nseg_prop[jj],x,xi_prop,tau_prop[jj], token_information)\n    log_beta_prop <- log_beta_prop+dmvnorm(beta_prop[,jj],fit$beta_mean, fit$beta_var, log = TRUE)\n    log_beta_prior_prop <- log_beta_prior_prop+\n      dmvnorm(beta_prop[,jj],rep(0,nbeta),diag(c(sigmasqalpha,rep(tau_prop[jj],nbasis))),\n              log = TRUE )#Prior Density of beta\n    log_tau_prior_prop <- log_tau_prior_prop-log(tau_up_limit) #Prior Density of Tausq\n    fhat <- fit$nu_mat %*% beta_prop[,jj]\n    # Ae = as.vector(fit$prdgrm*exp(-as.vector(fhat)))\n    # log_prop_spec_dens = -sum(fhat+Ae)  #Loglikelihood  at proposed values\n    log_prop_spec_dens <- whittle_like(fit, fhat, nseg_prop[jj])\n    loglike_prop = loglike_prop + log_prop_spec_dens\n  }\n  log_seg_prop = -log(nposs_seg) #Proposal for Segment choice\n  log_cut_prop = -log(nposs_cut) #Proposal for Cut point choice\n  #Evaluating prior density for cut points at proposed values\n  log_prior_cut_prop = 0\n  for (k in 1:(nexp_prop-1)){\n    if (k==1){\n      log_prior_cut_prop = -log(nobs-(nexp_prop-k+1)*tmin+1)\n    } else{\n      log_prior_cut_prop = log_prior_cut_prop-log(nobs-xi_prop[k-1]-(nexp_prop-k+1)*tmin+1)\n    }\n  }\n  #Calculating Log Proposal density at Proposed values\n  log_proposal_prop = log_beta_prop+log_seg_prop+log_move_prop+log_cut_prop\n  #Calculating Log Prior density at Proposed values\n  log_prior_prop = log_beta_prior_prop+log_tau_prior_prop+log_prior_cut_prop\n  #Calculating Target density at Proposed values\n  log_target_prop = loglike_prop+log_prior_prop\n  #*************************************************************\n  #CURRENT VALUES\n  #*************************************************************\n  #=======================================\n  #Evaluating the Likelihood, Proposal and Prior Densities at the Current values\n  #=======================================\n  #Beta Proposal and Prior\n  fit = post_beta(seg_cut,nseg_curr_temp[seg_cut],x,xi_curr_temp,tau_curr_temp[seg_cut],token_information)\n  if (nexp_curr ==1) {\n    log_beta_curr = dmvnorm(beta_curr_temp,fit$beta_mean,fit$beta_var,log=TRUE)\n    log_beta_prior_curr = dmvnorm(beta_curr_temp,rep(0,nbeta),\n                                  diag(c(sigmasqalpha,rep(tau_curr_temp[seg_cut],nbasis))),log=TRUE)\n  }else{\n    log_beta_curr = dmvnorm(beta_curr_temp[,seg_cut],fit$beta_mean,fit$beta_var,log=TRUE)\n    log_beta_prior_curr = dmvnorm(beta_curr_temp[,seg_cut],\n                                  rep(0,nbeta),diag(c(sigmasqalpha,rep(tau_curr_temp[seg_cut],nbasis))),log=TRUE)\n  }\n  log_tau_prior_curr = -log(tau_up_limit)\n  #Loglikelihood  at current values\n  if (nexp_curr ==1){\n    fhat = fit$nu_mat %*% beta_curr_temp\n  } else{\n    fhat = fit$nu_mat %*% beta_curr_temp[,seg_cut]\n  }\n  # Ae = as.vector(fit$prdgrm*exp(-as.vector(fhat)))\n  # log_curr_spec_dens = -sum(fhat+Ae)\n  log_curr_spec_dens = whittle_like(fit, fhat, nseg_curr_temp[seg_cut])\n  loglike_curr = log_curr_spec_dens\n  #Calculating Log proposal density at current values\n  log_proposal_curr = log_beta_curr+log_move_curr\n  #Evaluating  prior density for cut points at current values\n  log_prior_cut_curr = 0\n  for (k in 1:(nexp_curr-1)){\n    if (k==1){\n      log_prior_cut_curr = -log(nobs-(nexp_curr-k+1)*tmin+1)\n    } else{\n      log_prior_cut_curr = log_prior_cut_curr-log(nobs-xi_curr_temp[k-1]-(nexp_curr-k+1)*tmin+1)\n    }\n  }\n  if (nexp_curr == 1) log_prior_cut_curr = 0\n  #Calculating Priors at Current Vlaues\n  log_prior_curr = log_beta_prior_curr+log_tau_prior_curr+log_prior_cut_curr\n  #Evalulating Target densities at current values\n  log_target_curr = loglike_curr+log_prior_curr\n  met_rat = min(1,exp(log_target_prop-log_target_curr+log_proposal_curr-log_proposal_prop+log_jacobian))\n  list(met_rat = met_rat,nseg_prop = nseg_prop,xi_prop = xi_prop,tau_prop = tau_prop,\n       beta_prop = beta_prop)\n} # end function\n\nx0 <- 1:length(x)\nx <- lm(x ~ x0)$res\nnobs <- length(x)\ntt <- 1:nobs\nnbeta <- nbasis+1\ntoken_information <- list(nobs = nobs, var_inflate = var_inflate,\n                          nbasis = nbasis, nbeta = nbeta,\n                          sigmasqalpha= sigmasqalpha,\n                          prob_mm1= prob_mm1, tmin = tmin,\n                          tau_prior_a = tau_prior_a,\n                          tau_prior_b = tau_prior_b,\n                          tau_up_limit = tau_up_limit,\n                          step_size_max = step_size_max)\ntausq <- matrix(list(), nexp_max, 1)\nbeta <- matrix(list(), nexp_max,1)\nxi <- matrix(list(), nexp_max,1) #Cutpoint locations xi_1 is first cutpoint, xi_) is beginning of timeseries\nnseg <- matrix(list(), nexp_max,1) #Number of observations in each segment\nlog_spec_hat <- matrix(list(), nexp_max,1)\n\nfreq_hat <- (0:nfreq_hat)/(2*nfreq_hat)\nnu_mat_hat <- lin_basis_func(freq_hat, nbeta)\nfor (j in 1:nexp_max){\n  tausq[[j]] <- matrix(1, j, nloop+1)\n  beta[[j]] <- array(rep(1,nbeta*j*(nloop+1)), dim = c(nbeta,j,nloop+1))\n  xi[[j]] <- matrix(1,j,nloop+1)\n  nseg[[j]] <- matrix(1,j,nloop+1)\n  log_spec_hat[[j]] <- array(rep(0,(nfreq_hat+1)*j*(nloop+1)), dim = c(nfreq_hat+1,j,nloop+1))\n}\nnexp_curr <- rep(1, nloop+1)\nfor (j in 1:nexp_curr[1]){\n  tausq[[nexp_curr[1]]][j,1] <- runif(1)*tau_up_limit\n}\n\nfor (j in 1:nexp_curr[1]){\n  if (nexp_curr[1]==1){\n    xi[[nexp_curr[1]]][j,1] <- nobs\n    nseg[[nexp_curr[1]]][j,1] <- nobs\n  } else {\n    if (j==1){\n      nposs <- nobs - nexp_curr[1]*tmin+1\n      xi[[nexp_curr[1]]][j,1] <- tmin+sample(1:nposs,1,replace = TRUE)-1\n      nseg[[nexp_curr[1]]][j,1] <- xi[[nexp_curr[1]]][j,1]\n    } else if (j>1 & j < nexp_curr[1]){\n      nposs <- nobs-xi[[nexp_curr[1]]][j-1,1]-tmin*(nexp_curr[1]-j+1)+1\n      xi[[nexp_curr[1]]][j,1] <- tmin+sample(1:nposs,1,replace = TRUE)+xi[[nexp_curr[1]]][j-1,1]-1\n      nseg[[nexp_curr[1]]][j,1] <- xi[[nexp_curr[1]]][j,1]-xi[[nexp_curr[1]]][j-1,1]\n    } else {\n      xi[[nexp_curr[1]]][j,1] <- nobs\n      nseg[[nexp_curr[1]]][j,1] <- xi[[nexp_curr[1]]][j,1] - xi[[nexp_curr[1]]][j-1,1]\n    }\n  }\n}\nxi_temp <- xi[[nexp_curr[1]]][,1]\nnseg_temp <- nseg[[nexp_curr[1]]][,1]\ntau_temp <- tausq[[nexp_curr[1]]][,1]\nfor (j in 1:nexp_curr[1]){\n  fit <- post_beta(j,nseg_temp[j],x,xi_temp,tau_temp[j], token_information)\n  beta[[nexp_curr[1]]][,j,1] <- as.vector(rmvnorm(1, fit$beta_mean, fit$beta_var))\n}\nepsilon <- rep(0, nloop)\nmet_rat <- rep(0, nloop)\nRev_Jump <- 1\n\nfor (p in 1:nloop){\n  if(p %% 100 == 0){\n    cat(\"p:\", p, \" \\n\")\n  }\n  if (Rev_Jump == 1){\n    # BETWEEN MODEL MOVE\n    # Number of available segments\n    kk <- length(which(nseg[[nexp_curr[p]]][,p] > 2*tmin))\n    # Deciding on birth or death\n    if (kk == 0){ #Stay where you (if nexp_curr=1) or join segments if there are no available segments to cut\n      if(nexp_curr[p] == 1){\n        nexp_prop <- nexp_curr[p] # Stay where you are\n        log_move_prop <- 0\n        log_move_curr <- 0\n      }else{\n        nexp_prop <- nexp_curr[p] - 1 # join segments\n        log_move_prop <- 1\n        if (nexp_prop == 1){\n          log_move_curr <- 1\n        }else{\n          log_move_curr <- log(0.5)\n        }\n      }\n    } else{ # kk is not == 0\n      if (nexp_curr[p] == 1){\n        nexp_prop <- nexp_curr[p]+1\n        log_move_prop <- 0\n        if (nexp_prop == nexp_max){\n          log_move_curr <- 0\n        } else{\n          log_move_curr <- log(0.5)\n        }\n      } else if (nexp_curr[p] == nexp_max){\n        nexp_prop <- nexp_curr[p]-1\n        log_move_prop <- 0\n        if(nexp_prop == 1){\n          log_move_curr <- 0\n        } else{\n          log_move_curr <- log(0.5)\n        }\n      } else{\n        u <- runif(1)\n        if (u < .5){\n          nexp_prop <- nexp_curr[p]+1\n          if (nexp_prop==nexp_max) {\n            log_move_curr <- 0\n            log_move_prop <- log(0.5)\n          } else {\n            log_move_curr <- log(0.5)\n            log_move_prop <- log(0.5)\n          }\n        } else{\n          nexp_prop <- nexp_curr[p]-1\n          if (nexp_prop == 1){\n            log_move_curr <- 0\n            log_move_prop <- log(0.5)\n          } else{\n            log_move_curr <- log(0.5)\n            log_move_prop <- log(0.5)\n          }\n        }\n      }\n    } # end if (kk = 0)\n    xi_curr_temp <- xi[[nexp_curr[p]]][,p]\n    beta_curr_temp <- beta[[nexp_curr[p]]][,,p]\n    tau_curr_temp <- tausq[[nexp_curr[p]]][,p]\n    nseg_curr_temp <- nseg[[nexp_curr[p]]][,p]\n    if (nexp_prop < nexp_curr[p]){\n      #Death\n      death_step <- death(x,nexp_curr[p],nexp_prop,tau_curr_temp,xi_curr_temp,nseg_curr_temp,\n                          beta_curr_temp,log_move_curr,log_move_prop, token_information)\n      met_rat[p] <- death_step$met_rat\n      nseg_prop <- death_step$nseg_prop\n      xi_prop <- death_step$xi_prop\n      tau_prop <- death_step$tau_prop\n      beta_prop <- death_step$beta_prop\n    } else if (nexp_prop > nexp_curr[p]) {\n      # Birth\n      birth_step <- birth(x,nexp_curr[p],nexp_prop,tau_curr_temp,\n                          xi_curr_temp,nseg_curr_temp,beta_curr_temp,\n                          log_move_curr,log_move_prop,token_information)\n      met_rat[p] <- birth_step$met_rat\n      nseg_prop <- birth_step$nseg_prop\n      xi_prop <- birth_step$xi_prop\n      tau_prop <- birth_step$tau_prop\n      beta_prop <- birth_step$beta_prop\n    } else {\n      xi_prop <- xi[[nexp_curr[p]]][,p]\n      nseg_prop <- nseg[[nexp_curr[p]]][,p]\n      tau_prop <- tausq[[nexp_curr[p]]][,p]\n      beta_prop <- beta[[nexp_curr[p]]][,,p]\n      met_rat[p] <- 1\n    }\n    u <- runif(1)\n    if (u < met_rat[p]){\n      nexp_curr[p+1] <- nexp_prop\n      xi[[nexp_curr[p+1]]][,p+1] <- xi_prop\n      nseg[[nexp_curr[p+1]]][,p+1] <- nseg_prop\n      tausq[[nexp_curr[p+1]]][,p+1] <- tau_prop\n      beta[[nexp_curr[p+1]]][,,p+1] <- beta_prop\n    } else{\n      nexp_curr[p+1] <- nexp_curr[p]\n      xi[[nexp_curr[p+1]]][,p+1] <- xi[[nexp_curr[p+1]]][,p]\n      nseg[[nexp_curr[p+1]]][,p+1] <- nseg[[nexp_curr[p+1]]][,p]\n      tausq[[nexp_curr[p+1]]][,p+1] <- tausq[[nexp_curr[p+1]]][,p]\n      beta[[nexp_curr[p+1]]][,,p+1] <- beta[[nexp_curr[p+1]]][,,p]\n    }\n  } else { # Rev_Jump = 0\n    nexp_curr[p+1] <- nexp_curr[p]\n    xi[[nexp_curr[p+1]]][,p+1] <- xi[[nexp_curr[p+1]]][,p]\n    nseg[[nexp_curr[p+1]]][,p+1] <- nseg[[nexp_curr[p+1]]][,p]\n    tausq[[nexp_curr[p+1]]][,p+1] <- tausq[[nexp_curr[p+1]]][,p]\n    beta[[nexp_curr[p+1]]][,,p+1] <- beta[[nexp_curr[p+1]]][,,p]\n  }\n  #WITHIN MODEL MOVE\n  #Drawing a new cut point and beta simultaneously\n  #First draw the size of the move\n  xi_curr_temp <- xi[[nexp_curr[p+1]]][,p+1]\n  beta_curr_temp <- beta[[nexp_curr[p+1]]][,,p+1]\n  tau_temp <- tausq[[nexp_curr[p+1]]][,p+1]\n  nseg_curr_temp <- nseg[[nexp_curr[p+1]]][,p+1]\n  within_step <- within(x,nexp_curr[p+1],xi_curr_temp,beta_curr_temp,\n                        nseg_curr_temp,tau_temp,token_information)\n  epsilon[p] <- within_step$epsilon\n  xi_prop <- within_step$xi_prop\n  beta_prop <- within_step$beta_prop\n  nseg_new <- within_step$nseg_new\n  seg_temp <- within_step$seg_temp\n  u <- runif(1)\n  if (u < epsilon[p] || p==1){\n    if (nexp_curr[p+1] > 1){\n      for (j in seg_temp:(seg_temp+1)){\n        beta[[nexp_curr[p+1]]][,j,p+1] <- beta_prop[,j]\n        xi[[nexp_curr[p+1]]][j,p+1] <- xi_prop[j]\n        nseg[[nexp_curr[p+1]]][j,p+1] <- nseg_new[j]\n      }\n    } else{\n      beta[[nexp_curr[p+1]]][,1,p+1] <- beta_prop[,1]\n    }\n  } else {\n    beta[[nexp_curr[p+1]]][,,p+1] <- beta_curr_temp\n    xi[[nexp_curr[p+1]]][,p+1] <- xi_curr_temp\n    nseg[[nexp_curr[p+1]]][,p+1] <- nseg_curr_temp\n  }\n  # Drawing tausq\n  for (j in 1:nexp_curr[p+1]){\n    tau_a <- nbasis/2 + tau_prior_a\n    tau_b <- sum(beta[[nexp_curr[p+1]]][2:nbeta,j,p+1]^2)/2+tau_prior_b\n    u <- runif(1)\n    const1 <- pigamma(tau_up_limit, tau_a, tau_b)\n    const2 <- u*const1\n    tausq[[nexp_curr[p+1]]][j,p+1] <- qigamma(const2, tau_a, tau_b)\n  }\n  # Estimating Spectral Density\n  for (j in 1:nexp_curr[p+1]){\n    log_spec_hat[[nexp_curr[p+1]]][,j,p+1] <- nu_mat_hat %*% beta[[nexp_curr[p+1]]][,j,p+1]\n  }\n} # end gibbs loop\nfmean <- matrix(list(), nexp_max, 1)\nfor (j in 1:nexp_max){\n  fmean[[j]] <- matrix(1,length(freq_hat),j)\n}\n\nif (plotting == TRUE){\n\n  #Plots of individual Spectra\n  for (j in 1:nexp_max){\n    kk <- which(nexp_curr[(nwarmup+1):nloop] == j)\n    if (length(kk) != 0){\n      fmean[[j]] <- apply(log_spec_hat[[j]][,,kk+nwarmup],c(1,2), mean)\n      for (k in 1:j){\n        plot(freq_hat,fmean[[j]][,k], type = \"l\", main = paste(\"Log Spectral Density for component number\", k, \" in a mixture of\", j))\n        # lines(true_spec_dens[[k]]$freq,log(true_spec_dens[[k]]$spec),type = \"l\", col = \"red\")\n      }\n    }\n  }\n\n  # Plots of Partition Points\n  for (j in 1:nexp_max){\n    kk <- which(nexp_curr[(nwarmup+1):nloop] == j)\n    if (length(kk) != 0 & j > 1){\n      for (k in 1:(j-1)){\n        plot(xi[[j]][k,kk+nwarmup], type = \"l\", main = paste(\"Plot of\",k,\"th partition points\"))\n      }\n      for (k in 1:(j-1)){\n        hist(xi[[j]][k,kk+nwarmup])\n      }\n    }\n  }\n}\n\nz <- list(xi = xi,\n          log_spec_hat = log_spec_hat,\n          nloop = nloop,\n          nwarmup = nwarmup,\n          nexp_max = nexp_max,\n          x = x,\n          nexp_curr = nexp_curr,\n          nexp_max = nexp_max,\n          tmin = tmin,\n          sigmasqalpha = sigmasqalpha,\n          tau_prior_a = tau_prior_a,\n          tau_prior_b = tau_prior_b,\n          tau_up_limit = tau_up_limit,\n          prob_mm1 = prob_mm1,\n          step_size_max = step_size_max,\n          var_inflate = var_inflate,\n          nbasis = nbasis,\n          nfreq_hat = nfreq_hat)\nreturn(z)\n}\n",
    "created" : 1487717956381.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "4152509089",
    "id" : "A4A9380E",
    "lastKnownWriteTime" : 1487718323,
    "last_content_update" : 1487718323023,
    "path" : "~/Documents/BayesSpec/R/adaptspec.R",
    "project_path" : "R/adaptspec.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}